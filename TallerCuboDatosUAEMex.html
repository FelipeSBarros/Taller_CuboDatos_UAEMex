<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Manejando cubo de datos ráster en R  </title>
    <meta charset="utf-8" />
    <meta name="author" content="Felipe Sodré M. Barros   Instituto Superior Antonio Ruiz de Montoya Facultad de Ciencias Forestales de la Universidad Nacional de Misiones Argentina  felipesbarros.github.io" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Manejando cubo de datos ráster en R<br><br>
]
.author[
### Felipe Sodré M. Barros <br><br><i>Instituto Superior Antonio Ruiz de Montoya</i><br><i>Facultad de Ciencias Forestales de la Universidad Nacional de Misiones<i><br>Argentina<br><br>felipesbarros.github.io
]
.date[
### <br>28/11/2022
]

---


background-image: url(./img/UAEMex.png), url(./img/FacGeografía.jpg), url(./img/GeoLIBERO.png)
background-size: 200px, 200px, 240px
background-position: 00% 50%, 50% 50%, 95% 50%

class: center

# Manejando cubo de datos ráster en R

---



# Manejando cubo de datos ráster

## Contenido:

--

### 1. Contextualización:

### 2. Por qué cubo de datos?

### 3. Manejando Cubo de datos

### 4. Cubo de datos ráster

### 5. Cubo de datos vectorial

### 6. Creando un cubo de datos raster

### 7. Para estar atento

### Referencias y contactos

---

background-image: url(./img/EngolindoFumaca.png)
background-size: 950px
background-position: 10% 0%

class: inverse, center

# 1. Contextualización

---

# 1.1 "Engolindo Fumaça"

Proyecto de **periodísmo de datos** que buscó **identificar los efectos de los incendios forestales en el agravamiento de los casos de COVID19** en la Amazonia brasileña;
[Publicaciones](https://infoamazonia.org/project/engolindo-fumaca/)  

**Desafíos:**
* identificar patrón espacio-temporal de los incendios / **modelos espaciales** de materiales particulados (pm&lt;2.5ug);
* identificar municípios más afectados/expuestos a dichos materales;
* brindar conocimientos para la producción de reportajes periodísticos;

--

&lt;img src="https://media.giphy.com/media/wZnReyIOKg94mgokn5/giphy.gif" width="30%" /&gt;

---

class: inverse, middle, center

# 2. Por qué cubo de datos?

---

# 2. Por qué cubo de datos?

&lt;br&gt;
El paquete `raster` ([Hijmans 2022a](https://rspatial.org/raster/)) ha sido el dominante en el manejo de datos ráster en R. Es sin duda poderoso, flexible y escalable.
&lt;br&gt;

--

Tanto en el paquete `raster` cuanto en su sucesor `terra` ([Hijmans 2022b](https://rspatial.org/terra/)), la estructura (o modelo de dato) raster es una malla regular 2D (o un conjunto del mismo tipo - “raster stack”).
&lt;br&gt;
--
Acorde a [Pebesma &amp; Bivand](https://r-spatial.org/book/) se trata de la visión clásica de SIG:

&gt; "where the world is modelled as a set of layers, each representing a different theme."

&lt;br&gt;
--
Dichos autores llaman la atención al hecho de que actualmente tenemos acceso a datos poducidos en tiempo real (dinámicos) y nos forza a trabajar con el **parámetro  temporal** de los rasters (y raster stack) :: **cubo de datos**.
&lt;br&gt;
--
&lt;br&gt;
Aunque muchos se lo crean, un `raster stack` no refleja esa condición (espacio-temporal).

---

class: inverse

background-image: url(./img/cube1.png), url(./img/cube2.png)
background-size: 400px, 600px
background-position: 0% 0%, 95% 95%

---

class: inverse

background-image: url(http://brazildatacube.org/wp-content/uploads/2020/12/artigo-brazil-data-cube-inpe-1.jpg)
background-size: 800px
background-position: 50% 50%

# Disclaimer:

---

# 2.1 La extensión NetCDF

La estructura de datos más utitizada para almacenar cubos de datos (y no solo), es `NetCDF`, que basicamente se caracteriza por:

&gt; “binary data files that are portable across platforms and include metadata information in addition to the data sets.”

*Traducción*:  
&gt; “ser un archivo de dato binario indenpendiende de la plataforma computacional que inluye metadatos más allá del conjunto de datos.”

---

# 2.2 El paquete stars

El paquete [`stars`](https://cran.r-project.org/web/packages/stars/index.html) nasce para justamente permitir la *creacción*, *carga* y *manipulación* de cubo de datos *vectoriales* y *raster*.

Además, nos permite:  
* diferenciar `atributos` de las `dimensiones`  de los datos (espaciales / **temporal**);
* disempeño en los procesamientos;
* integración con `sf`, `raster` y `GDAL`;
* uso de matrices no regulares (*rotated*, *sheared*, *rectilinear* y *curvilinear*);
* sigue el princípio "tidyverse";

---

class: inverse, center

# 3. Manos a la obra:

---
background-image: url(https://dplyr.tidyverse.org/logo.png)
background-size: 100px
background-position: 95% 0%

# 3.1 Cargandos los paquetes necesarios

[`dplyr`](https://dplyr.tidyverse.org/) es un paquete que nos facilita la manipulación de datos;  
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
[`stars`](https://cran.r-project.org/web/packages/stars/index.html) es el paquete que vamos a ocupar para cargar y manipular los cubos de datos;
&lt;br&gt;
&lt;br&gt;


```r
library(stars)
library(dplyr)
```

---

# 3.2 Cargando los datos


```r
# Estados (provincias) de amazonia
uf &lt;- readRDS('./data/tidy/uf.rds')

# cubo de datos material particulado &lt; 2.5
(tseries &lt;- read_stars("./data/CAMS/pm25_daily_mean_2020.nc"))
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s), summary of first 1e+05 cells:
##                                   Min.  1st Qu. Median     Mean 3rd Qu.     Max. NA's
## pm25_daily_mean_2020.nc [ug] 0.2597396 5.280652 7.7442 8.823416 10.9463 166.1607 5681
## dimension(s):
##      from  to         offset  delta  refsys x/y
## x       1  78         -74.59    0.4      NA [x]
## y       1  62           6.16   -0.4      NA [y]
## time    1 366 2020-01-01 UTC 1 days POSIXct
```

---

# 3.3 Entendiendo la clase stars

Cada dimensión tiene un nombre;

Las dimensiones presentan los siguientes campos:

| **field** | **significado** |
|:---:|:---:|
| from | indice de origen (1) |
| to | index final (dim(x)[i]) |
| offset | el valor inicial para dicha dimensión (pixel boundary), si regular |
| delta | el valor de cambio de la dimensión, si regular |
| refsys | el sistema de referencia (proj4string, POSIXct, etc) |
| point | verdadero/falso; informa cuando el pixel se refiere a un punto |
| values | secuencia de valores para la dimensión, si irregular |

---

# Al respecto de los datos

Los datos que vamos a cargar ya fueron tratados y organizados: Los datos de las provincias fueron descargados usando [geobr](https://github.com/ipeaGIT/geobr) y filtrado por aquellos que hacen parte de la Amazonia brasileña ("Amazonia Legal");

Los modelos de material particulado fino (&lt; 2.5µg) fueron descargados del "[Copernicus Atmospheric Monitoring Service (CAMS)](https://atmosphere.copernicus.eu/charts/cams)" para todas las nueve horas de todos los días de 2020 y, para no manejarmos un conjunto de datos tan grande, vamos a trabajar con el valor medio diário.

---

# Para que sepan...

El paquete `stars` permite cargar los datos a partir de dos funcciones: `read_ncdf` and `read_stars`, siendo el ultimo usando `GDAL`.

---

# Haciendo algunos ajustes basicos


```r
sf::st_crs(tseries) &lt;- st_crs(uf)
tseries
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s), summary of first 1e+05 cells:
##                                   Min.  1st Qu. Median     Mean 3rd Qu.     Max. NA's
## pm25_daily_mean_2020.nc [ug] 0.2597396 5.280652 7.7442 8.823416 10.9463 166.1607 5681
## dimension(s):
##      from  to         offset  delta  refsys x/y
## x       1  78         -74.59    0.4  WGS 84 [x]
## y       1  62           6.16   -0.4  WGS 84 [y]
## time    1 366 2020-01-01 UTC 1 days POSIXct
```

---

# Identificando la dimensión "time"


```r
st_get_dimension_values(tseries, "time")
```

```
##   [1] "2020-01-01 UTC" "2020-01-02 UTC" "2020-01-03 UTC" "2020-01-04 UTC" "2020-01-05 UTC"
##   [6] "2020-01-06 UTC" "2020-01-07 UTC" "2020-01-08 UTC" "2020-01-09 UTC" "2020-01-10 UTC"
##  [11] "2020-01-11 UTC" "2020-01-12 UTC" "2020-01-13 UTC" "2020-01-14 UTC" "2020-01-15 UTC"
##  [16] "2020-01-16 UTC" "2020-01-17 UTC" "2020-01-18 UTC" "2020-01-19 UTC" "2020-01-20 UTC"
##  [21] "2020-01-21 UTC" "2020-01-22 UTC" "2020-01-23 UTC" "2020-01-24 UTC" "2020-01-25 UTC"
##  [26] "2020-01-26 UTC" "2020-01-27 UTC" "2020-01-28 UTC" "2020-01-29 UTC" "2020-01-30 UTC"
##  [31] "2020-01-31 UTC" "2020-02-01 UTC" "2020-02-02 UTC" "2020-02-03 UTC" "2020-02-04 UTC"
##  [36] "2020-02-05 UTC" "2020-02-06 UTC" "2020-02-07 UTC" "2020-02-08 UTC" "2020-02-09 UTC"
##  [41] "2020-02-10 UTC" "2020-02-11 UTC" "2020-02-12 UTC" "2020-02-13 UTC" "2020-02-14 UTC"
##  [46] "2020-02-15 UTC" "2020-02-16 UTC" "2020-02-17 UTC" "2020-02-18 UTC" "2020-02-19 UTC"
##  [51] "2020-02-20 UTC" "2020-02-21 UTC" "2020-02-22 UTC" "2020-02-23 UTC" "2020-02-24 UTC"
##  [56] "2020-02-25 UTC" "2020-02-26 UTC" "2020-02-27 UTC" "2020-02-28 UTC" "2020-02-29 UTC"
##  [61] "2020-03-01 UTC" "2020-03-02 UTC" "2020-03-03 UTC" "2020-03-04 UTC" "2020-03-05 UTC"
##  [66] "2020-03-06 UTC" "2020-03-07 UTC" "2020-03-08 UTC" "2020-03-09 UTC" "2020-03-10 UTC"
##  [71] "2020-03-11 UTC" "2020-03-12 UTC" "2020-03-13 UTC" "2020-03-14 UTC" "2020-03-15 UTC"
##  [76] "2020-03-16 UTC" "2020-03-17 UTC" "2020-03-18 UTC" "2020-03-19 UTC" "2020-03-20 UTC"
##  [81] "2020-03-21 UTC" "2020-03-22 UTC" "2020-03-23 UTC" "2020-03-24 UTC" "2020-03-25 UTC"
##  [86] "2020-03-26 UTC" "2020-03-27 UTC" "2020-03-28 UTC" "2020-03-29 UTC" "2020-03-30 UTC"
##  [91] "2020-03-31 UTC" "2020-04-01 UTC" "2020-04-02 UTC" "2020-04-03 UTC" "2020-04-04 UTC"
##  [96] "2020-04-05 UTC" "2020-04-06 UTC" "2020-04-07 UTC" "2020-04-08 UTC" "2020-04-09 UTC"
## [101] "2020-04-10 UTC" "2020-04-11 UTC" "2020-04-12 UTC" "2020-04-13 UTC" "2020-04-14 UTC"
## [106] "2020-04-15 UTC" "2020-04-16 UTC" "2020-04-17 UTC" "2020-04-18 UTC" "2020-04-19 UTC"
## [111] "2020-04-20 UTC" "2020-04-21 UTC" "2020-04-22 UTC" "2020-04-23 UTC" "2020-04-24 UTC"
## [116] "2020-04-25 UTC" "2020-04-26 UTC" "2020-04-27 UTC" "2020-04-28 UTC" "2020-04-29 UTC"
## [121] "2020-04-30 UTC" "2020-05-01 UTC" "2020-05-02 UTC" "2020-05-03 UTC" "2020-05-04 UTC"
## [126] "2020-05-05 UTC" "2020-05-06 UTC" "2020-05-07 UTC" "2020-05-08 UTC" "2020-05-09 UTC"
## [131] "2020-05-10 UTC" "2020-05-11 UTC" "2020-05-12 UTC" "2020-05-13 UTC" "2020-05-14 UTC"
## [136] "2020-05-15 UTC" "2020-05-16 UTC" "2020-05-17 UTC" "2020-05-18 UTC" "2020-05-19 UTC"
## [141] "2020-05-20 UTC" "2020-05-21 UTC" "2020-05-22 UTC" "2020-05-23 UTC" "2020-05-24 UTC"
## [146] "2020-05-25 UTC" "2020-05-26 UTC" "2020-05-27 UTC" "2020-05-28 UTC" "2020-05-29 UTC"
## [151] "2020-05-30 UTC" "2020-05-31 UTC" "2020-06-01 UTC" "2020-06-02 UTC" "2020-06-03 UTC"
## [156] "2020-06-04 UTC" "2020-06-05 UTC" "2020-06-06 UTC" "2020-06-07 UTC" "2020-06-08 UTC"
## [161] "2020-06-09 UTC" "2020-06-10 UTC" "2020-06-11 UTC" "2020-06-12 UTC" "2020-06-13 UTC"
## [166] "2020-06-14 UTC" "2020-06-15 UTC" "2020-06-16 UTC" "2020-06-17 UTC" "2020-06-18 UTC"
## [171] "2020-06-19 UTC" "2020-06-20 UTC" "2020-06-21 UTC" "2020-06-22 UTC" "2020-06-23 UTC"
## [176] "2020-06-24 UTC" "2020-06-25 UTC" "2020-06-26 UTC" "2020-06-27 UTC" "2020-06-28 UTC"
## [181] "2020-06-29 UTC" "2020-06-30 UTC" "2020-07-01 UTC" "2020-07-02 UTC" "2020-07-03 UTC"
## [186] "2020-07-04 UTC" "2020-07-05 UTC" "2020-07-06 UTC" "2020-07-07 UTC" "2020-07-08 UTC"
## [191] "2020-07-09 UTC" "2020-07-10 UTC" "2020-07-11 UTC" "2020-07-12 UTC" "2020-07-13 UTC"
## [196] "2020-07-14 UTC" "2020-07-15 UTC" "2020-07-16 UTC" "2020-07-17 UTC" "2020-07-18 UTC"
## [201] "2020-07-19 UTC" "2020-07-20 UTC" "2020-07-21 UTC" "2020-07-22 UTC" "2020-07-23 UTC"
## [206] "2020-07-24 UTC" "2020-07-25 UTC" "2020-07-26 UTC" "2020-07-27 UTC" "2020-07-28 UTC"
## [211] "2020-07-29 UTC" "2020-07-30 UTC" "2020-07-31 UTC" "2020-08-01 UTC" "2020-08-02 UTC"
## [216] "2020-08-03 UTC" "2020-08-04 UTC" "2020-08-05 UTC" "2020-08-06 UTC" "2020-08-07 UTC"
## [221] "2020-08-08 UTC" "2020-08-09 UTC" "2020-08-10 UTC" "2020-08-11 UTC" "2020-08-12 UTC"
## [226] "2020-08-13 UTC" "2020-08-14 UTC" "2020-08-15 UTC" "2020-08-16 UTC" "2020-08-17 UTC"
## [231] "2020-08-18 UTC" "2020-08-19 UTC" "2020-08-20 UTC" "2020-08-21 UTC" "2020-08-22 UTC"
## [236] "2020-08-23 UTC" "2020-08-24 UTC" "2020-08-25 UTC" "2020-08-26 UTC" "2020-08-27 UTC"
## [241] "2020-08-28 UTC" "2020-08-29 UTC" "2020-08-30 UTC" "2020-08-31 UTC" "2020-09-01 UTC"
## [246] "2020-09-02 UTC" "2020-09-03 UTC" "2020-09-04 UTC" "2020-09-05 UTC" "2020-09-06 UTC"
## [251] "2020-09-07 UTC" "2020-09-08 UTC" "2020-09-09 UTC" "2020-09-10 UTC" "2020-09-11 UTC"
## [256] "2020-09-12 UTC" "2020-09-13 UTC" "2020-09-14 UTC" "2020-09-15 UTC" "2020-09-16 UTC"
## [261] "2020-09-17 UTC" "2020-09-18 UTC" "2020-09-19 UTC" "2020-09-20 UTC" "2020-09-21 UTC"
## [266] "2020-09-22 UTC" "2020-09-23 UTC" "2020-09-24 UTC" "2020-09-25 UTC" "2020-09-26 UTC"
## [271] "2020-09-27 UTC" "2020-09-28 UTC" "2020-09-29 UTC" "2020-09-30 UTC" "2020-10-01 UTC"
## [276] "2020-10-02 UTC" "2020-10-03 UTC" "2020-10-04 UTC" "2020-10-05 UTC" "2020-10-06 UTC"
## [281] "2020-10-07 UTC" "2020-10-08 UTC" "2020-10-09 UTC" "2020-10-10 UTC" "2020-10-11 UTC"
## [286] "2020-10-12 UTC" "2020-10-13 UTC" "2020-10-14 UTC" "2020-10-15 UTC" "2020-10-16 UTC"
## [291] "2020-10-17 UTC" "2020-10-18 UTC" "2020-10-19 UTC" "2020-10-20 UTC" "2020-10-21 UTC"
## [296] "2020-10-22 UTC" "2020-10-23 UTC" "2020-10-24 UTC" "2020-10-25 UTC" "2020-10-26 UTC"
## [301] "2020-10-27 UTC" "2020-10-28 UTC" "2020-10-29 UTC" "2020-10-30 UTC" "2020-10-31 UTC"
## [306] "2020-11-01 UTC" "2020-11-02 UTC" "2020-11-03 UTC" "2020-11-04 UTC" "2020-11-05 UTC"
## [311] "2020-11-06 UTC" "2020-11-07 UTC" "2020-11-08 UTC" "2020-11-09 UTC" "2020-11-10 UTC"
## [316] "2020-11-11 UTC" "2020-11-12 UTC" "2020-11-13 UTC" "2020-11-14 UTC" "2020-11-15 UTC"
## [321] "2020-11-16 UTC" "2020-11-17 UTC" "2020-11-18 UTC" "2020-11-19 UTC" "2020-11-20 UTC"
## [326] "2020-11-21 UTC" "2020-11-22 UTC" "2020-11-23 UTC" "2020-11-24 UTC" "2020-11-25 UTC"
## [331] "2020-11-26 UTC" "2020-11-27 UTC" "2020-11-28 UTC" "2020-11-29 UTC" "2020-11-30 UTC"
## [336] "2020-12-01 UTC" "2020-12-02 UTC" "2020-12-03 UTC" "2020-12-04 UTC" "2020-12-05 UTC"
## [341] "2020-12-06 UTC" "2020-12-07 UTC" "2020-12-08 UTC" "2020-12-09 UTC" "2020-12-10 UTC"
## [346] "2020-12-11 UTC" "2020-12-12 UTC" "2020-12-13 UTC" "2020-12-14 UTC" "2020-12-15 UTC"
## [351] "2020-12-16 UTC" "2020-12-17 UTC" "2020-12-18 UTC" "2020-12-19 UTC" "2020-12-20 UTC"
## [356] "2020-12-21 UTC" "2020-12-22 UTC" "2020-12-23 UTC" "2020-12-24 UTC" "2020-12-25 UTC"
## [361] "2020-12-26 UTC" "2020-12-27 UTC" "2020-12-28 UTC" "2020-12-29 UTC" "2020-12-30 UTC"
## [366] "2020-12-31 UTC"
```

---

background-image: url(https://i.gifer.com/Fudg.gif)
background-size: 100px
background-position: 95% 22%

# 4. Cubo de datos ráster

Ya que estamos trabajando con la dimensión tiempo, es más que natural necesitar seleccionar una fecha o un rango de fechas específicas.

Lo podríamos hacer usando el índice (**posición**) en el cual el dato ocupa en el tiempo:

---

# 4. Cubo de datos ráster

## 4.1 usando índices: `slice`

* seleccionando los seis primeros días de 2020:


```r
tseries %&gt;% 
  slice(index = 1:6, along = "time")
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s):
##                                   Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## pm25_daily_mean_2020.nc [ug] 0.2618084 5.846485 8.298655 9.383015 11.44545 72.64661 1656
## dimension(s):
##      from to         offset  delta  refsys x/y
## x       1 78         -74.59    0.4  WGS 84 [x]
## y       1 62           6.16   -0.4  WGS 84 [y]
## time    1  6 2020-01-01 UTC 1 days POSIXct
```

---

# 4. Cubo de datos ráster



```r
tseries %&gt;% 
  slice(index = 1:6, along = "time") %&gt;% 
  plot()
```

![](TallerCuboDatosUAEMex_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---

El `plot` de objetos de la clase `stars` tiene, por defecto, escala de color gris y está justado(?) (*stretched*) usando cuantil de **todas las dimensiones** (“histogram equalization”). Pero podemos passar otros estílos usando el parametro `breaks`. Ejemplo:  =  `breaks = "equal"`.


```r
tseries %&gt;% 
  slice(index = 1:6, along = "time") %&gt;% 
  plot(breaks = "equal")
```

![](TallerCuboDatosUAEMex_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

---

# 4. Cubo de datos ráster

## 4.3 usando `filter`

Hasta ahora todo bien, pero necesitamos trabajar con la información 'temporal' y no por índice. O sea, si sabemos que el período de incendios en Amazonia suele empezar en Julio y terminar en Octubre, se nos complicaria identificar en qué posición (indice) están las imágenes que necesitamos.

Para manejar los datos por fechas, podemos usar la función `filter`.
ej.: filtrando los seis primeros días de Julio:


```r
tseries %&gt;% 
  filter(
    time &gt;= lubridate::ymd('2020-07-01'),
    time &lt;= lubridate::ymd('2020-07-06')
    )
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s):
##                                   Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## pm25_daily_mean_2020.nc [ug] 0.6771017 4.623379 6.792793 7.967353 9.811567 110.5665 1656
## dimension(s):
##      from to         offset  delta  refsys x/y
## x       1 78         -74.59    0.4  WGS 84 [x]
## y       1 62           6.16   -0.4  WGS 84 [y]
## time    1  6 2020-07-01 UTC 1 days POSIXct
```

---

# 4. Cubo de datos ráster

## 4.4 "estadísticas" relacionadas a la dimensión tiemporal: `aggregate`

Necesitamos saber el valor promedio de polución a cada semana (siete días) para un rango de fecha. Para eso usamos la función  `aggregate`;


```r
tseries %&gt;% filter(
  time &gt;= lubridate::ymd('2020-07-01'),
  time &lt; lubridate::ymd('2020-11-01')
  ) %&gt;% aggregate(by = "7 days", FUN = mean)
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s):
##                               Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## pm25_daily_mean_2020.nc  0.6540222 6.769163 10.40521 16.83849 19.55275 485.7011 4968
## dimension(s):
##      from to         offset  delta  refsys x/y
## time    1 18 2020-07-01 -03 7 days POSIXct    
## x       1 78         -74.59    0.4  WGS 84 [x]
## y       1 62           6.16   -0.4  WGS 84 [y]
```

---

# 4. Cubo de datos ráster

## 4.4 "estadísticas" relacionadas a la dimensión tiemporal y espacial

Bien, pero necesitamos estos valores promedios por UF.

Basta usar la misma función (`aggregate`) aplicado a un dato espacial:


```r
tseries %&gt;% filter(
  time &gt;= lubridate::ymd('2020-07-01'),
  time &lt; lubridate::ymd('2020-11-01')
  ) %&gt;% aggregate(by = "7 days", FUN = mean) %&gt;%
  aggregate(by = uf, FUN = mean) %&gt;% 
  plot()
```

```
## Warning: plotting the first 9 out of 18 attributes; use max.plot = 18 to plot all
```

![](TallerCuboDatosUAEMex_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---

# 5. Cubo de datos **vectorial**?

Qué resultado tengo del `aggregate` espacial?


```r
(datos_agregados_uf &lt;- tseries %&gt;% 
  aggregate(by = uf, FUN = mean))
```

```
## stars object with 2 dimensions and 1 attribute
## attribute(s):
##                                  Min.  1st Qu.   Median     Mean 3rd Qu.     Max. NA's
## pm25_daily_mean_2020.nc [ug] 1.034596 7.050042 9.733505 12.83667 13.6511 125.5489  366
## dimension(s):
##      from  to         offset  delta  refsys point
## geom    1   9             NA     NA  WGS 84 FALSE
## time    1 366 2020-01-01 UTC 1 days POSIXct    NA
##                                                             values
## geom MULTIPOLYGON (((-63.32721...,...,MULTIPOLYGON (((-54.89485...
## time                                                          NULL
```

---

background-image: url(./img/cube3.png)
background-size: 600px
background-position: 50% 50%

# 5. Cubo de datos vectorial

---

# 5. Cubo de datos vectorial

Y se maneja igual...


```r
datos_agregados_uf %&gt;% filter(
  time &gt;= lubridate::ymd('2020-07-01'),
  time &lt; lubridate::ymd('2020-12-01')
  ) %&gt;% aggregate(by = "1 months", FUN = mean)
```

```
## stars object with 2 dimensions and 1 attribute
## attribute(s):
##                              Min.  1st Qu.   Median     Mean 3rd Qu.     Max. NA's
## pm25_daily_mean_2020.nc  3.749945 9.826947 14.42175 18.20641 22.4809 69.24747    5
## dimension(s):
##      from to  refsys point                                                        values
## time    1  5 POSIXct    NA                                     2020-07-01,...,2020-11-01
## geom    1  9  WGS 84 FALSE MULTIPOLYGON (((-63.32721...,...,MULTIPOLYGON (((-54.89485...
```

---

# 5. Cubo de datos vectorial

Y se maneja igual II...


```r
datos_agregados_uf %&gt;% filter(
  time &gt;= lubridate::ymd('2020-07-01'),
  time &lt; lubridate::ymd('2020-12-01')
  ) %&gt;% plot()
```

---

# 6. Creando un cubo de datos raster

Vamos a ocupar los datos del 01-01-2020 como ejemplo y converterlo a raster.


```r
(r &lt;- tseries %&gt;% slice(index = 1:30, along = "time") %&gt;% as("SpatRaster"))
```

```
## class       : SpatRaster 
## dimensions  : 62, 78, 30  (nrow, ncol, nlyr)
## resolution  : 0.4, 0.4  (x, y)
## extent      : -74.59, -43.39, -18.64, 6.16  (xmin, xmax, ymin, ymax)
## coord. ref. : lon/lat WGS 84 (EPSG:4326) 
## source(s)   : memory
## names       : time2~01-01, time2~01-02, time2~01-03, time2~01-04, time2~01-05, time2~01-06, ... 
## min values  :   0.7638272,   0.2618084,    2.005264,   0.6795541,   0.9549108,   0.6271348, ... 
## max values  :  50.8284035,  72.6466141,   61.733257,  54.2623367,  71.2163773,  41.1043892, ...
```

---

# Convirtiendo un objeto raster para stars

Una vez que tenemos al dato raster cargado como `raster`, podremos convertirlos al modelo `stars`:


```r
(s &lt;- st_as_stars(r))
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s):
##                      Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## time2020-01-01  0.2597396 5.564454 8.109292 9.239033 11.46879 243.6578 8280
## dimension(s):
##      from to offset delta refsys                            values x/y
## x       1 78 -74.59   0.4 WGS 84                              NULL [x]
## y       1 62   6.16  -0.4 WGS 84                              NULL [y]
## band    1 30     NA    NA     NA time2020-01-01,...,time2020-01-30
```

---

# Organizando el metadatos

Pero, tendremos que organizar los metadatos. Primero vamos a añadir un nombre al atributo:


```r
names(s) &lt;- "Polución"
st_get_dimension_values(s, 'band')
```

```
##  [1] "time2020-01-01" "time2020-01-02" "time2020-01-03" "time2020-01-04" "time2020-01-05"
##  [6] "time2020-01-06" "time2020-01-07" "time2020-01-08" "time2020-01-09" "time2020-01-10"
## [11] "time2020-01-11" "time2020-01-12" "time2020-01-13" "time2020-01-14" "time2020-01-15"
## [16] "time2020-01-16" "time2020-01-17" "time2020-01-18" "time2020-01-19" "time2020-01-20"
## [21] "time2020-01-21" "time2020-01-22" "time2020-01-23" "time2020-01-24" "time2020-01-25"
## [26] "time2020-01-26" "time2020-01-27" "time2020-01-28" "time2020-01-29" "time2020-01-30"
```

---

# Cambiando el metadato

Vamos a ocupar el `st_set_dimensions` para cambiar la dimensión `band` al modelo de fechas y cambiamos su nombre:


```r
(s &lt;- st_set_dimensions(s,
                  # which = 3,
                  which = 'band',
                  values = lubridate::date('2020-01-01') + 
                    c(0:30),
                  names = 'fechas'))
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s):
##                Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Polución  0.2597396 5.564454 8.109292 9.239033 11.46879 243.6578 8280
## dimension(s):
##        from to offset delta refsys                                              values x/y
## x         1 78 -74.59   0.4 WGS 84                                                NULL [x]
## y         1 62   6.16  -0.4 WGS 84                                                NULL [y]
## fechas    1 30     NA    NA   Date [2020-01-01,2020-01-02),...,[2020-01-30,2020-01-31)
```

---

# confirmando los valores de dimensión 'fechas'


```r
st_get_dimension_values(s, 'fechas')
```

```
##  [1] "2020-01-01" "2020-01-02" "2020-01-03" "2020-01-04" "2020-01-05" "2020-01-06"
##  [7] "2020-01-07" "2020-01-08" "2020-01-09" "2020-01-10" "2020-01-11" "2020-01-12"
## [13] "2020-01-13" "2020-01-14" "2020-01-15" "2020-01-16" "2020-01-17" "2020-01-18"
## [19] "2020-01-19" "2020-01-20" "2020-01-21" "2020-01-22" "2020-01-23" "2020-01-24"
## [25] "2020-01-25" "2020-01-26" "2020-01-27" "2020-01-28" "2020-01-29" "2020-01-30"
```

---

# Guardando y cargando el cubo de forma sensilla

Si pretendien seguir usando R, podrás guardar en el fomrato `rds`:


```r
saveRDS(s["Polución",,,], "cubo_polucion.rds")
```

---

# Guardando y cargando el cubo de forma sensilla

Para cargar al nuestro proyecto R, usamos la función `readRDS`:


```r
readRDS("cubo_polucion.rds")
```

```
## stars object with 3 dimensions and 1 attribute
## attribute(s):
##                Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## Polución  0.2597396 5.564454 8.109292 9.239033 11.46879 243.6578 8280
## dimension(s):
##        from to offset delta refsys                                              values x/y
## x         1 78 -74.59   0.4 WGS 84                                                NULL [x]
## y         1 62   6.16  -0.4 WGS 84                                                NULL [y]
## fechas    1 30     NA    NA   Date [2020-01-01,2020-01-02),...,[2020-01-30,2020-01-31)
```

---

# 7. Para estar atento

## Procesamiento "on-disk"

El paquete `stars` permite realizar **procesamiento de los cubos de datos sin tenerlos cargados en la memoria**. Para eso se ocupan los objetos `stars_proxy` que contienen solamente los metadatos y apunta hacia los archivos en el disco duro.

De esta manera, el procesamiento se da de forma "tardía" (*lazily*): La lectura y los procesamientos solamente son realizados en el momento en el que los pixels son realmente necesarios (Ej. en el plot o cuando se guarda el resultado en el disco).

Más informacioines en [`stars_proxy`](https://r-spatial.github.io/stars/articles/stars2.html).

---

# Referencias y contactos 

## Referencias 

- [Documentación `stars`](https://r-spatial.github.io/stars)

- [Libro online: Spatial Data Science](https://r-spatial.org/book/07-Introsf.html#package-stars)

## contactos

- [felipe.b4rros@gmail.com](felipe.b4rros@gmail.com)

- [felipesbarros.github.io](https://felipesbarros.github.io)

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "rainbow",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
